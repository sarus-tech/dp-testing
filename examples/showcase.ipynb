{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![View On GitHub](https://img.shields.io/badge/View_in_Github-grey?logo=github)](https://gitlab.com/your_username/your_repository/-/blob/main/examples/showcase.ipynb)\n",
    "![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)\n",
    "\n",
    "# Cracking the Code: Practical Methods for Testing Differential Privacy\n",
    "\n",
    "In sarus we build privacy safe analytics tools and one of our initiatives is an open source library called [qrlew](https://qrlew.readthedocs.io/en/latest/). Qrlew is used to turn sql queries into privacy save ones under the framework of [Differential Privacy (DP)](https://en.wikipedia.org/wiki/Differential_privacy). As the library was maturing and more features were supported we needed to test that the results generated from DP rewritten queries were actually coherent with the theory. Thanks to the approach presented here, we actually found a bug in Qrlew and corrected it straight away. This motivated us to open source our testing methodology and tools to the developers and researchers community so, let's dive in.\n",
    "\n",
    "To build trustworthy data analysis systems, it’s important to verify that differential privacy mechanisms truly provide the privacy guarantees they promise. While there have been other attempts to create open-source libraries for differential privacy testing—such as Google’s Differential Privacy [Stochastic Tester](https://github.com/google/differential-privacy/blob/main/cc/testing/README.md), [DP Auditorium](https://github.com/google/differential-privacy/tree/main/python/dp_auditorium), and potentially others, our approach focuses on making it exceptionally easy to create datasets that resemble real-life scenarios and to effortlessly test them with any SQL query. In this ready-to-run Jupyter notebook on Google Colab, we introduce practical methods for testing differential privacy results. Adopting the perspective of an adversary attempting to breach privacy, we demonstrate how hypothesis testing, specifically using the Neyman–Pearson lemma, can assess whether a mechanism effectively protects individual entries in a dataset.\n",
    "\n",
    "Here, we also introduce the dp_tester repository that implements these testing techniques, showcasing how straightforward it is to apply them to your own differential privacy mechanisms. Through hands-on code examples and interactive experiments, you’ll see how easy it is to empirically validate and verify the privacy properties of your implementations using our tools.\n",
    "\n",
    "This practical approach bridges the gap between theoretical guarantees and real-world applications, empowering developers and researchers to ensure their differential privacy solutions are both robust and effective—all within a convenient and user-friendly Colab environment.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [How to test DP results](#intro)\n",
    "- [Experimental settings & Results](#dp_testing_in_practice)\n",
    "  1) [Dataset Generation](#dataset_generation)\n",
    "  2) [Collecting DP Results](#collecting_dp_results)\n",
    "  3) [Partitioning Results](#paritioning_results)\n",
    "  4) [Compute Empirical Epsilons](#compute_empirical_epsilons)\n",
    "- [Conclusions](#conclusions)\n",
    "- [References](#theoretical_backgroud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## How to test DP results \n",
    "\n",
    "Differential privacy (DP) offers a robust mathematical framework to ensure that the output of a mechanism $\\mathcal{M}$ does not compromise the privacy of any individual in a dataset. Formally a mechanism $\\mathcal{M}$ is ($\\epsilon$,$\\delta$)-differential private if:\n",
    "\n",
    "$$ Pr \\left[ \\mathcal M\\left( D_0 \\right) \\in S\\right] \\leq e^{\\epsilon} Pr\\left[ \\mathcal M\\left(D_1\\right)\\in S \\right] + \\delta $$\n",
    "\n",
    "$\\forall \\; D_0$ and $D_1$, where $|D_0-D_1| \\leq 1$ (neighboring datasets differing by at most one individual), $\\forall \\; S$ any possible output set. $\\epsilon$ is the privacy loss parameter, and $\\delta$ is a small failure probability.\n",
    "\n",
    "### Adversary’s Perspective\n",
    "\n",
    "To test whether a mechanism truly satisfies differential privacy, we can adopt the perspective of an adversary attempting to determine whether a specific user is included in a dataset. The adversary’s goal is to distinguish between outputs from $D_0$ (with the user) and $D_1$ (without the user).\n",
    "\n",
    "### Hypothesis Testing Framework\n",
    "\n",
    "This scenario can be framed as a hypothesis testing problem using the Neyman–Pearson lemma:\n",
    "- Null Hypothesis ($H_0$): The output Y originates from $\\mathcal{M}(D_0)$.\n",
    "- Alternative Hypothesis ($H_1$): The output Y originates from $\\mathcal{M}(D_1)$.\n",
    "\n",
    "The Neyman–Pearson lemma is a fundamental result in statistical hypothesis testing that provides a method for constructing the most powerful test for distinguishing between two simple hypotheses and has applications across different domains such as in medicine, physics, economy etc. The lemma states that for a given significance level  $\\alpha$  (the maximum allowable probability of a Type I error), the test for deciding between  $H_0$  and  $H_1$  is the one that rejects  $H_0$  in favor of  $H_1$  when the likelihood ratio $\\Lambda(Y)$  exceeds a certain threshold $\\tau$:\n",
    "$$\\Lambda(Y) = \\frac{f_1(Y)}{f_0(Y)} \\geq \\tau$$ \n",
    "The threshold $\\tau$ is chosen so that the test has the desired significance level $\\alpha$:\n",
    "$$ \\alpha = Pr_{H_0}[\\Lambda(Y) \\geq \\tau] $$\n",
    "The lemma guarantees that among all possible tests with significance level  $\\alpha$ , the likelihood ratio test is the most powerful—it has the highest probability of correctly rejecting  $H_0$  when  $H_1$  is true (i.e., the lowest Type II error  $\\beta$ i.e. )\n",
    "\n",
    "### Computing The Empirical Epsilon\n",
    "\n",
    "In our scenario, in order to compute the likelihood ratio, a type I and type II error for our hypothesis test we can consider a sub region of the output space delimited by an arbitrary $S_{\\tau}$, it implies that we can write the following from the formal definition above:\n",
    "$$ Pr \\left[ \\mathcal M\\left( D_0 \\right) \\in S_{\\tau}\\right] \\leq e^{\\epsilon} Pr\\left[ \\mathcal M\\left(D_1\\right)\\in S_{\\tau} \\right] + \\delta \\quad \\quad \\forall \\; D_0, D_1, |D_0-D_1| \\leq 1, \\quad \\forall \\; S_{\\tau} $$\n",
    "where $S_{\\tau} \\in \\{\\frac{Pr \\left[ \\mathcal M\\left( D_0 \\right) \\in S\\right]} {Pr\\left[ \\mathcal M\\left(D_1\\right)\\in S \\right]} \\leq \\tau \\}$.\n",
    "\n",
    "Now we can define the probability of false positive as $FP_{\\tau}=1-Pr\\left[ \\mathcal M\\left( D_0 \\right) \\in S_{\\tau}\\right]$ (type I error), when the null hypothesis is true but rejected, and the probability of False Negative (type II error) as $FN_{\\tau}=Pr\\left[ \\mathcal M\\left( D_1 \\right) \\in S_{\\tau}\\right]$. In the graph below there is a schematic representation of our FP and FN definitions ($P_0$ and $P_1$ are respectively $Pr\\left[ \\mathcal M\\left( D_o \\right) \\in S_{\\tau}\\right]$ and $Pr\\left[ \\mathcal M\\left( D_1 \\right) \\in S_{\\tau}\\right]$).\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"figure_fp-fn_2.png\" width=\"400\" alt=\"\" />\n",
    "</div>\n",
    "\n",
    "If we plug our type I and type II error in the original definition and we re-arrange a bit in order to isolate $\\epsilon$ we get the following:\n",
    "$$ \\epsilon \\geq ln\\left(\\frac {1 - \\delta - FP_{\\tau}} {FN_{\\tau}}\\right) \\quad \\quad \\forall \\; D_0, D_1, |D_0-D_1| \\leq 1, \\quad \\forall \\; S_{\\tau}  $$\n",
    "Since this relation is valid for all possible neighboring datasets It is true as well for a set neighboring datasets and moreover it is true as well for the neighboring dataset that maximizes the empirical epsilon. Thus, it is implied that:\n",
    "$$ \\epsilon \\geq \\epsilon^{*} = \\max_{\\tau, \\pi} \\left[ ln\\left(\\frac {1 - \\delta - FP_{\\tau}} {FN_{\\tau}}\\right) \\right] \\quad \\quad \\forall \\; (D_0, D_1) \\in \\{\\pi_1, \\pi_2, ... \\pi_n\\}, \\pi_i = (D^i_0, D^i_1), |D^i_0-D^i_1| \\leq 1, \\quad \\forall \\; S_{\\tau} $$\n",
    "This relation tells us that the $\\epsilon^{*}$ should never overcome the actual $\\epsilon$ used by the mechanism otherwise there are potential vulnerability in the mechanism implementation. Since $FP_{\\tau}$ and $FN_{\\tau}$ are unknown probabilities which we need to estimate.\n",
    "$$ \\epsilon \\stackrel{?}{\\geq} \\hat{\\epsilon^{*}} = \\max_{\\tau, \\pi} \\left[ ln\\left(\\frac {1 - \\delta - \\widehat{FP_{\\tau}}} {\\widehat{FN_{\\tau}}}\\right) \\right] \\quad \\quad \\forall \\; D_0, D_1 \\in \\{\\pi_1, \\pi_2, ... \\pi_n\\}, \\pi_i = (D^i_0-D^i_1), |D^i_0-D^i_1| \\leq 1, \\quad \\forall \\; S_{\\tau}  $$\n",
    "where the $?$ indicates that this formula may not hold if $\\widehat{FP_{\\tau}}$ and $\\widehat{FN_{\\tau}}$ are poor estimators. We use empirical probability estimators for $FP_{\\tau}$ and $FN_{\\tau}$, we count a the number of occurrences in a specific bucket over a sufficiency large number or experiments (on the order of 10k or more). In this way the analysis presented so far is conducted for each individual bucket thus the $\\hat{\\epsilon^{*}}$ is the maximum across all partitions along with all neighboring datasets.\n",
    "\n",
    "### Interpreting the Results\n",
    "\n",
    "The test fails if $\\hat{\\epsilon^{*}}$ is significantly greater that $\\epsilon$. This indicates that the mechanism does not meet the privacy criteria, revealing a potential vulnerability that needs to be addressed. If $\\hat{\\epsilon^{*}}$ across all tested dataset pairs is less than or equal to the claimed $\\epsilon$, the mechanism appears to uphold the differential privacy guarantee for those cases.\n",
    "This procedure has some limitations, since we cannot test all possible neighboring datasets, passing the test does not conclusively prove that the mechanism is differentially private in all scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dp_testing_in_practice\"></a>\n",
    "## Experimental settings\n",
    "\n",
    "In this section we go from theory to practice in 4 steps:\n",
    "1) **Dataset Generation**: a brief description of the datasets caracteristics we generated for the experiments.\n",
    "2) **Collecting DP Results**: description on how we collected our DP results.\n",
    "3) **Partitioning Results**: description and tools for partitioning the output space into buckets needed for computing counts.\n",
    "4) **Compute Empirical Epsilons**: here we compute empirical epsilons for all partitions and compare it to the actual epsilon used for the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dataset_generation\"></a>\n",
    "### Dataset Generation\n",
    "\n",
    "In the following cells we create a postgres database and we push our $D_0$ and $D_1$ datasets which differ by the removal of the data relative to 1 user.\n",
    "\n",
    "The dataset consists in 2 tables:\n",
    "- `users`: it has 100 lines each related to a distinct user. The columns are `id: (int) UNIQUE` identifying the user and `income: (float)` is a random number drawn from $\\mathcal{N}(\\mu=40000, \\sigma=10000)$\n",
    "- `transactions`: it has 10000 entries, The columns are `id: (int) UNIQUE` the transaction id, `user_id: (int)` the user who is making the transaction (it is one of the user in users), `spent: (float)` is a random number drawn from $\\mathcal{U}(a=5,b=500)$, `store_id: (int)` identifier of the store, there are 200 unique stores, `other_id: (int)` is the identifier of some arbitrary feature of the user, there are 10 possible unique other ids.\n",
    "\n",
    "\n",
    "In the `transactions` table the user `0` has at most 500 contributions while the remaining transactions are split uniformly among the remaining users. Moreover, the user `0` likes to make transactions in all stores and he likes them differently: the higher the store_id the higher the frequency the user `0` likes to make transactions. In other wards user `0` affects all the stores in different way. The other users don't have any particular preference among the stores so the pick them randomly. Lastly, each user affects only 1 `other_id`.\n",
    "\n",
    "$D_0$ is pushed to the default postgres schema while $D_1$ which has no information about the user with id=0 is pushed to the `D_1` schema. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Create a postgres database\n",
    "# Inspired by https://colab.research.google.com/github/tensorflow/io/blob/master/docs/tutorials/postgresql.ipynb#scrollTo=YUj0878jPyz7\n",
    "!sudo apt-get -y -qq update\n",
    "!sudo apt-get -y -qq install postgresql-14\n",
    "# Start postgresql server\n",
    "!sudo sed -i \"s/port = 5432/port = 5433/g\" /etc/postgresql/14/main/postgresql.conf\n",
    "!sudo service postgresql start\n",
    "# Set password\n",
    "!sudo -u postgres psql -U postgres -c \"ALTER USER postgres PASSWORD 'pyqrlew-db'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_tester.generate_datasets import generate_D_0_dataset, generate_adj_datasets\n",
    "from dp_tester.constants import D_1\n",
    "\n",
    "generate_D_0_dataset()\n",
    "generate_adj_datasets(D_1, user_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"collecting_dp_results\"></a>\n",
    "### Collecting DP Results\n",
    "\n",
    "To collect DP results, we use the `dp_results_from_sql_query` function which uses `SqlAlchemyQueryExecutor` query executor to submit a query to the database, we use `PyqrlewDpRewriter` to compile the query with differential privacy with the provided privacy parameters\n",
    "and the we use `PyqrlewTableRenamer` to change tables qualifying name to forward the query towards $D_1$.\n",
    "\n",
    "The DP query is generated ones and is submitted many times (as much as indicated by `runs` parameter) to both $D_0$ and $D_1$.\n",
    "Then results for each of the 2 dataset are stored in dictionary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_tester.results_collector import dp_results_from_sql_query\n",
    "from dp_tester.query_executors import SqlAlchemyQueryExecutor\n",
    "from dp_tester.dp_rewriters import PyqrlewDpRewriter\n",
    "from dp_tester.table_renamers import PyqrlewTableRenamer\n",
    "from dp_tester.constants import D_0\n",
    "import json\n",
    "\n",
    "query = (\n",
    "    \"SELECT store_id, SUM(spent) AS spent_per_store FROM transactions GROUP BY store_id\"\n",
    ")\n",
    "epsilon = 1.0\n",
    "delta = 1e-4\n",
    "runs = 5000\n",
    "\n",
    "query_executor = SqlAlchemyQueryExecutor()\n",
    "dp_rewriter = PyqrlewDpRewriter(engine=query_executor.engine)\n",
    "table_renamer = PyqrlewTableRenamer(dp_rewriter.dataset)\n",
    "\n",
    "results = dp_results_from_sql_query(\n",
    "    non_dp_query=query,\n",
    "    epsilon=epsilon,\n",
    "    delta=delta,\n",
    "    runs=runs,\n",
    "    dp_rewriter=dp_rewriter,\n",
    "    query_executor=query_executor,\n",
    "    table_renamer=table_renamer,\n",
    "    d_0=D_0,\n",
    "    adjacent_ds=[D_1],\n",
    ")\n",
    "\n",
    "# save results\n",
    "with open(\"results.json\", \"w\") as outfile:\n",
    "    json.dump(obj=results, fp=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"paritioning_results\"></a>\n",
    "### Partitioning Results\n",
    "\n",
    "Results structure can be very variate depending on the query and for a single query there are many ways to its results into buckets.\n",
    "Here, we attempt to be generic and leave to the user the decision on how to partition the results into buckets. We have a generic function that\n",
    "iterate over results and applies a user provided function which given the a single query results it returns a list of integers each associated\n",
    "to the index of the corresponding bucket.\n",
    "\n",
    "This approach should provide great flexibility while still remaining generic. \n",
    "\n",
    "For our case each query creates a list of row where each row is: `group_id, quantity`. For results of this structure we have implemented\n",
    "such function as follow:\n",
    "\n",
    "#### Buckets\n",
    "Let's say that `quantity` is defined as $x \\in \\mathbb{R} \\cup \\{ \\emptyset \\} $ and `group_id` as $g\\in G = \\{ g_1, g_2, \\dotsc, g_{M-1}\\} \\cup \\{\\emptyset\\}$. We can discretize the space of $x$ in a set of $ I = \\{ I_1, I_2, \\dotsc, I_{N-1} \\} \\cup \\{ \\emptyset \\}$ intervals where each interval $I_i$ is defined as: $I_i = [ a_{i},\\ b_i ), \\quad \\text{for } i = 1, 2, \\dotsc, N-1$, where N is the number of bins in which the space is discretized. The result space is therefore defined as  \n",
    "\n",
    "$$ S = \\{G \\times I \\} \\cup \\{ \\emptyset \\} $$ \n",
    "\n",
    "where the cross product $G \\times I$ is the set paris $\\{(g, I_i) | g\\in G, I_i \\in I\\}$. So results are partitioned into $M \\times N$ partitions or buckets. The method `generate_buckets` of the `QuantityOverGroups` object does exactly this. It uses the results to find quantities minimum and maximum in order then to generate NBINS intervals.\n",
    "\n",
    "#### Counts\n",
    "\n",
    "The function `results_to_bucket_ids` takes all the overall results and a used defined function, in this case provided by `QuantityOverGroups` object which associate an query results into bucket indexes as follow:\n",
    "for each group id (store id in our case), we return the index of the corresponding bucket if the pair (group id, quantity) is present in the result other wise the index of the last bucket is returned (the one relative to the $\\emptyset$). Notice that we take into account as well for groups that don't appear in the results. For each query results a list MxN bucket index is returned.\n",
    "\n",
    "As a next step `counts_from_indexes` counts the indexes or in other wards, the number of occurrences of a particular result in a specific bucket.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_tester.generate_datasets import N_STORES\n",
    "from dp_tester.partitioners import QuantityOverGroups\n",
    "from dp_tester.analyzer import results_to_bucket_ids, counts_from_indexes\n",
    "from dp_tester.typing import OverallResults\n",
    "import typing as t\n",
    "\n",
    "\n",
    "NBINS = 20\n",
    "\n",
    "# read results\n",
    "with open(\"results.json\") as infile:\n",
    "    results = t.cast(OverallResults, json.load(infile))\n",
    "\n",
    "partitioner = QuantityOverGroups(groups=list(range(N_STORES)))\n",
    "partitioner.generate_buckets(results, n_float_buckets=NBINS)\n",
    "bucket_ids = results_to_bucket_ids(results, partitioner.bucket_id)\n",
    "\n",
    "counts_d_0 = counts_from_indexes(bucket_ids[D_0], len(partitioner.buckets))\n",
    "counts_d_1 = counts_from_indexes(bucket_ids[D_1], len(partitioner.buckets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compute_empirical_epsilons\"></a>\n",
    "### Compute Empirical Epsilon\n",
    "\n",
    "Now we that we have counts for each bucket we can estimate false positive and false negative errors and therefore compute the empirical epsilon.\n",
    "Let's first compute the ratio between counts and sort them:\n",
    "\n",
    "$$ \\frac {c^{D_0}_{0}}{c^{D_1}_{0}} \\geq \\frac {c^{D_0}_{1}}{c^{D_1}_{1}} \\geq  ... \\geq \\frac {c^{D_0}_{n-1}}{c^{D_1}_{n-1}}$$ \n",
    "\n",
    "where $n$ is the $M \\times N$-th bucket and $\\sum_i c^{D_0}_{i} = \\sum_i c^{D_1}_{i} = C$ where C is the number or runs times the number of buckets. We can define a Neyman–Pearson lemma threshold as follow:\n",
    "\n",
    "$$ \\widehat{FP_{\\tau_i}} = 1-\\sum_{j\\leq\\tau_i}\\frac{c^{D_0}_{j}}{C};\\qquad \\widehat{FP_{\\tau_i}} = \\sum_{j\\leq\\tau_i} \\frac{c^{D_1}_{j}}{C} $$\n",
    "\n",
    "Where C is total number of runs, $ \\tau_i = \\bigcup_{j>i} S_j $ in which S is sorted b where we vary the threshold across the results space. \n",
    "\n",
    "The empirical epsilon be computed as:\n",
    "\n",
    "$$ \\hat{\\epsilon^{*}_T} = \\max_{\\tau_i, \\pi} \\left[ ln\\left(\\frac {1 - \\delta - \\widehat{FP_{\\tau_i}}} {\\widehat{FN_{\\tau_i}}}\\right), ln\\left(\\frac {1 - \\delta - \\widehat{FN_{\\tau_i}}} {\\widehat{FP_{\\tau_i}}}\\right) \\right]$$\n",
    "\n",
    "notice a couple of things:\n",
    "1) by switching $D_0$ and $D_1$ we can include in our test all cases where a user has been added to the dataset. This has been already computed and is sufficient to include the second term in the max arguments where $\\widehat{FP_{\\tau_i}}$ and $\\widehat{FN_{\\tau_i}}$ are replaced.\n",
    "\n",
    "2) The $T$ in $\\hat{\\epsilon^{*}_T}$ indicates that we are considering only buckets with at least $T$ counts: $\\sum_{j\\leq i}\\frac{c^{D_0}_{j}}{C} \\geq T$ and $\\sum_{j\\leq i}\\frac{c^{D_1}_{j}}{C} \\geq T$. We would like to avoid buckets with very few counts since we suppose that the error in estimating FPs and FNs is significantly high and it can falsify our test. One can either increase the number of runs with the hope that to populate a bit more such buckets or can calibrate $T$. $T$ can be calibrated by running an experiment such as counting the lines of the `users` table. The output is a response from a Laplace mechanism with sensibility of 1, this is a very simple mechanism for which $\\hat{\\epsilon^{*}_T}$ and $\\epsilon$ are the closest. T would be the lowest possible for which the test passes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_tester.analyzer import empirical_epsilon\n",
    "\n",
    "COUNT_THRESHOLD = 5\n",
    "\n",
    "empirical_eps = empirical_epsilon(\n",
    "    counts_d_0, counts_d_1, delta=delta, counts_threshold=COUNT_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"Epsilon used during the experiment: {epsilon}\")\n",
    "print(f\"Max empirical epsilon found: {empirical_eps}\")\n",
    "print(f\"Did the test passed? {empirical_eps < epsilon}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the test passes as we wanted to show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusions\"></a>\n",
    "## Conclusions\n",
    "\n",
    "In this notebook, we showed a methodology for testing any differentially private results using the principles of hypothesis testing and Neyman–Pearson lemma. We also introduced the `dp_testing` library with tools to easily experiment with realistic datasets creation, to obtain differentially private results from it and to analyze such results with hypothesis testing framework. Thanks to these tools we were able to find a problem with one of our DP mechanism implementation and to fix it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"theoretical_backgroud\"></a>\n",
    "## References\n",
    "\n",
    "- [Differencial Privacy](https://maxkasy.github.io/home/files/other/ML_Econ_Oxford/differential_privacy.pdf)\n",
    "- [Wilson et al.](https://arxiv.org/abs/1909.01917)\n",
    "- [Kairouz et al.](https://arxiv.org/abs/1311.0776)\n",
    "- [Milad Nasr et al.](https://arxiv.org/abs/2101.04535)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
